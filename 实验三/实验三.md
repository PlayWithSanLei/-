## Trying out a simple learner

Before we try to build our deep learning models, let's make sure we can learn something using a simple linear model.

1.  加载数据集


```python
import pandas as pd
from tensorflow.keras.utils import get_file
import nb_utils
```

```python
#emotion_csv = get_file('text_emotion.csv', 
#                       'https://www.crowdflower.com/wp-content/uploads/2016/07/text_emotion.csv')
emotion_df = pd.read_csv('./data/text_emotion.csv')
```


```python
emotion_df.head()
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
    .dataframe tbody tr th {
        vertical-align: top;
    }
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>tweet_id</th>
      <th>sentiment</th>
      <th>author</th>
      <th>content</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1956967341</td>
      <td>empty</td>
      <td>xoshayzers</td>
      <td>@tiffanylue i know  i was listenin to bad habi...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1956967666</td>
      <td>sadness</td>
      <td>wannamama</td>
      <td>Layin n bed with a headache  ughhhh...waitin o...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1956967696</td>
      <td>sadness</td>
      <td>coolfunky</td>
      <td>Funeral ceremony...gloomy friday...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1956967789</td>
      <td>enthusiasm</td>
      <td>czareaquino</td>
      <td>wants to hang out with friends SOON!</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1956968416</td>
      <td>neutral</td>
      <td>xkilljoyx</td>
      <td>@dannycastillo We want to trade with someone w...</td>
    </tr>
  </tbody>
</table>
</div>
```python
emotion_df['sentiment'].value_counts()
```


    neutral       8638
    worry         8459
    happiness     5209
    sadness       5165
    love          3842
    surprise      2187
    fun           1776
    relief        1526
    hate          1323
    empty          827
    enthusiasm     759
    boredom        179
    anger          110
    Name: sentiment, dtype: int64


```python
from sklearn.linear_model import SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_score
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

VOCAB_SIZE = 50000

tfidf_vec = TfidfVectorizer(max_features=VOCAB_SIZE)
label_encoder = LabelEncoder()

X = tfidf_vec.fit_transform(emotion_df['content'])
y = label_encoder.fit_transform(emotion_df['sentiment'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
```


```python
bayes = MultinomialNB()
bayes.fit(X_train, y_train)
predictions = bayes.predict(X_test)
precision_score(predictions, y_test, average='micro')
```


    0.2802272727272727


```python
classifiers = {'sgd': SGDClassifier(loss='hinge'),
               'svm': SVC(),
               'random_forest': RandomForestClassifier()}

for lbl, clf in classifiers.items():
    clf.fit(X_train, y_train)
    predictions = clf.predict(X_test)
    print(lbl, precision_score(predictions, y_test, average='micro'))

# 7:25s
```

    sgd 0.32515151515151514
    svm 0.34719696969696967
    random_forest 0.32825757575757575


## Checking what our model learned

Our linear models appear to be learning something more powerful than "pick the most popular category".  We can take a quick look at which words they find the most correlated with each category before moving on to our neural network.


```python
from scipy.sparse import eye
d = eye(len(tfidf_vec.vocabulary_))
word_pred = bayes.predict_proba(d)
inverse_vocab = {idx: word for word, idx in tfidf_vec.vocabulary_.items()}
```


```python
from collections import Counter, defaultdict
by_cls = defaultdict(Counter)
for word_idx, pred in enumerate(word_pred):
    for class_idx, score in enumerate(pred):
        cls = label_encoder.classes_[class_idx]
        by_cls[cls][inverse_vocab[word_idx]] = score
```


```python
for k in by_cls:
    words = [x[0] for x in by_cls[k].most_common(5)]
    print(k, ':', ' '.join(words))
```

    anger : confuzzled fridaaaayyyyy aaaaaaaaaaa transtelecom filthy
    boredom : squeaking ouuut cleanin sooooooo candyland3
    empty : _cheshire_cat_ bethsybsb conversating kimbermuffin less_than_3
    enthusiasm : lena_distractia foolproofdiva attending krisswouldhowse tatt
    fun : xbox bamboozle sanctuary oldies toodaayy
    happiness : excited woohoo excellent yay wars
    hate : hate hates suck fucking zomberellamcfox
    love : love mothers mommies moms loved
    neutral : www painting souljaboytellem link frenchieb
    relief : finally relax mastered relief inspiration
    sadness : sad sadly cry cried miss
    surprise : surprise wow surprised wtf surprisingly
    worry : worried poor throat hurts sick


## 使用卷积神经网络训练模型

## Training a deep model

Now that we've seen how well a simple linear model can do, let's see if we can do any better with a deep learning model.  In this case, we don't have an excessive amount of training data: this constrains the models we can train effectively: use too big of a model, and we'll end up overfitting our data.

We'll start with a CNN.


```python
from itertools import chain
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

```


```python
chars = list(sorted(set(chain(*emotion_df['content']))))
char_to_idx = {ch: idx for idx, ch in enumerate(chars)}
max_sequence_len = max(len(x) for x in emotion_df['content'])

char_vectors = []
for txt in emotion_df['content']:
    vec = np.zeros((max_sequence_len, len(char_to_idx)))
    vec[np.arange(len(txt)), [char_to_idx[ch] for ch in txt]] = 1
    char_vectors.append(vec)
char_vectors = np.asarray(char_vectors)
char_vectors = pad_sequences(char_vectors)
labels = label_encoder.transform(emotion_df['sentiment'])



def split(lst):
    training_count = int(0.9 * len(char_vectors))
    return lst[:training_count], lst[training_count:]

training_char_vectors, test_char_vectors = split(char_vectors)
training_labels, test_labels = split(labels)

char_vectors.shape
```


    (40000, 167, 100)


```python
len(char_vectors)
```


    40000


```python
from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras import regularizers

def create_char_cnn_model(num_chars, max_sequence_len, num_labels):
    char_input = Input(shape=(max_sequence_len, num_chars), name='input')
    
    conv_1x = Conv1D(128, 6, activation='relu', padding='valid')(char_input)
    max_pool_1x = MaxPooling1D(6)(conv_1x)
    conv_2x = Conv1D(256, 6, activation='relu', padding='valid')(max_pool_1x)
    max_pool_2x = MaxPooling1D(6)(conv_2x)

    flatten = Flatten()(max_pool_2x)
    dense = Dense(128, 
                  activation='relu',
                  kernel_regularizer=regularizers.l2(0.01))(flatten)
    preds = Dense(num_labels, activation='softmax')(dense)

    model = Model(char_input, preds)
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['acc'])
    return model

char_cnn_model = create_char_cnn_model(len(char_to_idx), char_vectors.shape[1], len(label_encoder.classes_))
char_cnn_model.summary()
```

    WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
    Instructions for updating:
    Call initializer instance with the dtype argument instead of passing it to the constructor
    Model: "model"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input (InputLayer)           [(None, 167, 100)]        0         
    _________________________________________________________________
    conv1d (Conv1D)              (None, 162, 128)          76928     
    _________________________________________________________________
    max_pooling1d (MaxPooling1D) (None, 27, 128)           0         
    _________________________________________________________________
    conv1d_1 (Conv1D)            (None, 22, 256)           196864    
    _________________________________________________________________
    max_pooling1d_1 (MaxPooling1 (None, 3, 256)            0         
    _________________________________________________________________
    flatten (Flatten)            (None, 768)               0         
    _________________________________________________________________
    dense (Dense)                (None, 128)               98432     
    _________________________________________________________________
    dense_1 (Dense)              (None, 13)                1677      
    =================================================================
    Total params: 373,901
    Trainable params: 373,901
    Non-trainable params: 0
    _________________________________________________________________

```python
char_cnn_model.fit(training_char_vectors, training_labels, epochs=20, batch_size=1024)
char_cnn_model.evaluate(test_char_vectors, test_labels)
```

    Epoch 1/20
    36000/36000 [==============================] - 4s 120us/sample - loss: 3.2683 - acc: 0.2368
    Epoch 2/20
    36000/36000 [==============================] - 2s 65us/sample - loss: 2.3942 - acc: 0.2442
    Epoch 3/20
    36000/36000 [==============================] - 2s 67us/sample - loss: 2.1999 - acc: 0.2504
    Epoch 4/20
    36000/36000 [==============================] - 2s 63us/sample - loss: 2.1539 - acc: 0.2500
    Epoch 5/20
    36000/36000 [==============================] - 2s 62us/sample - loss: 2.1208 - acc: 0.2607
    Epoch 6/20
    36000/36000 [==============================] - 2s 62us/sample - loss: 2.0926 - acc: 0.2741
    Epoch 7/20
    36000/36000 [==============================] - 2s 62us/sample - loss: 2.0725 - acc: 0.2843
    Epoch 8/20
    36000/36000 [==============================] - 2s 62us/sample - loss: 2.0483 - acc: 0.2974
    Epoch 9/20
    36000/36000 [==============================] - 2s 62us/sample - loss: 2.0354 - acc: 0.3044
    Epoch 10/20
    36000/36000 [==============================] - 2s 66us/sample - loss: 2.0144 - acc: 0.3111
    Epoch 11/20
    36000/36000 [==============================] - 2s 65us/sample - loss: 1.9997 - acc: 0.3218
    Epoch 12/20
    36000/36000 [==============================] - 2s 64us/sample - loss: 1.9892 - acc: 0.3246
    Epoch 13/20
    36000/36000 [==============================] - 2s 62us/sample - loss: 1.9725 - acc: 0.3321
    Epoch 14/20
    36000/36000 [==============================] - 2s 62us/sample - loss: 1.9613 - acc: 0.3389
    Epoch 15/20
    36000/36000 [==============================] - 2s 61us/sample - loss: 1.9512 - acc: 0.3397
    Epoch 16/20
    36000/36000 [==============================] - 2s 62us/sample - loss: 1.9414 - acc: 0.3470
    Epoch 17/20
    36000/36000 [==============================] - 2s 62us/sample - loss: 1.9265 - acc: 0.3537
    Epoch 18/20
    36000/36000 [==============================] - 2s 61us/sample - loss: 1.9199 - acc: 0.3549
    Epoch 19/20
    36000/36000 [==============================] - 2s 62us/sample - loss: 1.9081 - acc: 0.3598
    Epoch 20/20
    36000/36000 [==============================] - 2s 61us/sample - loss: 1.9006 - acc: 0.3609
    4000/4000 [==============================] - 0s 119us/sample - loss: 2.0419 - acc: 0.3375
    
    [2.041916464805603, 0.3375]


```python
from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Concatenate , LSTM
from keras.models import Model


def create_char_cnn_model(num_chars, max_sequence_len, num_labels):
    char_input = Input(shape=(max_sequence_len, num_chars), name='input')
    
    layers = []
    for window in (5, 6, 7):
        conv_1x = Conv1D(128, window, activation='relu', padding='valid')(char_input)
        max_pool_1x = MaxPooling1D(window)(conv_1x)
        dropout_1x = Dropout(0.3)(max_pool_1x)
        conv_2x = Conv1D(128, window, activation='relu', padding='valid')(dropout_1x)
        max_pool_2x = MaxPooling1D(window)(conv_2x)
        dropout_2x = Dropout(0.3)(max_pool_2x)
        layers.append(dropout_2x)

    if len(layers) > 1:
        merged = Concatenate(axis=1)(layers)
    else:
        merged = layers[0]

    dropout = Dropout(0.3)(merged)
    
    flatten = Flatten()(dropout)
    dense = Dense(128, activation='relu')(flatten)
    preds = Dense(num_labels, activation='softmax')(dense)

    model = Model(char_input, preds)
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['acc'])
    return model

char_cnn_model = create_char_cnn_model(len(char_to_idx), char_vectors.shape[1], len(label_encoder.classes_))
char_cnn_model.summary()
```

    WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.
    
    Using TensorFlow backend.


    Model: "model_1"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    input (InputLayer)              (None, 167, 100)     0                                            
    __________________________________________________________________________________________________
    conv1d_1 (Conv1D)               (None, 163, 128)     64128       input[0][0]                      
    __________________________________________________________________________________________________
    conv1d_3 (Conv1D)               (None, 162, 128)     76928       input[0][0]                      
    __________________________________________________________________________________________________
    conv1d_5 (Conv1D)               (None, 161, 128)     89728       input[0][0]                      
    __________________________________________________________________________________________________
    max_pooling1d_1 (MaxPooling1D)  (None, 32, 128)      0           conv1d_1[0][0]                   
    __________________________________________________________________________________________________
    max_pooling1d_3 (MaxPooling1D)  (None, 27, 128)      0           conv1d_3[0][0]                   
    __________________________________________________________________________________________________
    max_pooling1d_5 (MaxPooling1D)  (None, 23, 128)      0           conv1d_5[0][0]                   
    __________________________________________________________________________________________________
    dropout_1 (Dropout)             (None, 32, 128)      0           max_pooling1d_1[0][0]            
    __________________________________________________________________________________________________
    dropout_3 (Dropout)             (None, 27, 128)      0           max_pooling1d_3[0][0]            
    __________________________________________________________________________________________________
    dropout_5 (Dropout)             (None, 23, 128)      0           max_pooling1d_5[0][0]            
    __________________________________________________________________________________________________
    conv1d_2 (Conv1D)               (None, 28, 128)      82048       dropout_1[0][0]                  
    __________________________________________________________________________________________________
    conv1d_4 (Conv1D)               (None, 22, 128)      98432       dropout_3[0][0]                  
    __________________________________________________________________________________________________
    conv1d_6 (Conv1D)               (None, 17, 128)      114816      dropout_5[0][0]                  
    __________________________________________________________________________________________________
    max_pooling1d_2 (MaxPooling1D)  (None, 5, 128)       0           conv1d_2[0][0]                   
    __________________________________________________________________________________________________
    max_pooling1d_4 (MaxPooling1D)  (None, 3, 128)       0           conv1d_4[0][0]                   
    __________________________________________________________________________________________________
    max_pooling1d_6 (MaxPooling1D)  (None, 2, 128)       0           conv1d_6[0][0]                   
    __________________________________________________________________________________________________
    dropout_2 (Dropout)             (None, 5, 128)       0           max_pooling1d_2[0][0]            
    __________________________________________________________________________________________________
    dropout_4 (Dropout)             (None, 3, 128)       0           max_pooling1d_4[0][0]            
    __________________________________________________________________________________________________
    dropout_6 (Dropout)             (None, 2, 128)       0           max_pooling1d_6[0][0]            
    __________________________________________________________________________________________________
    concatenate_1 (Concatenate)     (None, 10, 128)      0           dropout_2[0][0]                  
                                                                     dropout_4[0][0]                  
                                                                     dropout_6[0][0]                  
    __________________________________________________________________________________________________
    dropout_7 (Dropout)             (None, 10, 128)      0           concatenate_1[0][0]              
    __________________________________________________________________________________________________
    flatten_1 (Flatten)             (None, 1280)         0           dropout_7[0][0]                  
    __________________________________________________________________________________________________
    dense_1 (Dense)                 (None, 128)          163968      flatten_1[0][0]                  
    __________________________________________________________________________________________________
    dense_2 (Dense)                 (None, 13)           1677        dense_1[0][0]                    
    ==================================================================================================
    Total params: 691,725
    Trainable params: 691,725
    Non-trainable params: 0
    __________________________________________________________________________________________________

```python
char_cnn_model.fit(training_char_vectors, training_labels, epochs=20, batch_size=1024)
char_cnn_model.evaluate(test_char_vectors, test_labels)
```

    WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.
    
    Epoch 1/20
    36000/36000 [==============================] - 4s 109us/step - loss: 2.2139 - acc: 0.2296
    Epoch 2/20
    36000/36000 [==============================] - 3s 84us/step - loss: 2.1366 - acc: 0.2493
    Epoch 3/20
    36000/36000 [==============================] - 3s 83us/step - loss: 2.1280 - acc: 0.2485
    Epoch 4/20
    36000/36000 [==============================] - 3s 85us/step - loss: 2.1046 - acc: 0.2578
    Epoch 5/20
    36000/36000 [==============================] - 3s 84us/step - loss: 2.0775 - acc: 0.2752
    Epoch 6/20
    36000/36000 [==============================] - 3s 84us/step - loss: 2.0535 - acc: 0.2862
    Epoch 7/20
    36000/36000 [==============================] - 3s 85us/step - loss: 2.0271 - acc: 0.2983
    Epoch 8/20
    36000/36000 [==============================] - 3s 85us/step - loss: 2.0057 - acc: 0.3058
    Epoch 9/20
    36000/36000 [==============================] - 3s 84us/step - loss: 1.9818 - acc: 0.3167
    Epoch 10/20
    36000/36000 [==============================] - 3s 84us/step - loss: 1.9620 - acc: 0.3234
    Epoch 11/20
    36000/36000 [==============================] - 3s 84us/step - loss: 1.9366 - acc: 0.3347
    Epoch 12/20
    36000/36000 [==============================] - 3s 84us/step - loss: 1.9188 - acc: 0.3393
    Epoch 13/20
    36000/36000 [==============================] - 3s 85us/step - loss: 1.9032 - acc: 0.3505
    Epoch 14/20
    36000/36000 [==============================] - 3s 85us/step - loss: 1.8808 - acc: 0.3548
    Epoch 15/20
    36000/36000 [==============================] - 3s 86us/step - loss: 1.8588 - acc: 0.3650
    Epoch 16/20
    36000/36000 [==============================] - 3s 86us/step - loss: 1.8400 - acc: 0.3708
    Epoch 17/20
    36000/36000 [==============================] - 3s 85us/step - loss: 1.8256 - acc: 0.3760
    Epoch 18/20
    36000/36000 [==============================] - 3s 85us/step - loss: 1.8035 - acc: 0.3851
    Epoch 19/20
    36000/36000 [==============================] - 3s 85us/step - loss: 1.7871 - acc: 0.3874
    Epoch 20/20
    36000/36000 [==============================] - 3s 85us/step - loss: 1.7612 - acc: 0.4017
    4000/4000 [==============================] - 1s 180us/step
    
    [2.1699029293060303, 0.2800000011920929]

## Featurizing and preparing our data

Just like we did when computing word embeddings, we want to featurize our data so we can classify it effectively.


```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import one_hot
import nb_utils
import pandas as pd

VOCAB_SIZE = 50000
tokenizer = Tokenizer(num_words=VOCAB_SIZE)
tokenizer.fit_on_texts(emotion_df['content'])
```


```python
import os
import re
import numpy as np
import gensim

CACHE_DIR = os.path.expanduser('~/.cache/dl-cookbook')

def download(url):
    filename = os.path.join(CACHE_DIR, re.sub('[^a-zA-Z0-9.]+', '_', url))
    if os.path.exists(filename):
        return filename
    else:
        os.system('mkdir -p "%s"' % CACHE_DIR)
        assert os.system('wget -O "%s" "%s"' % (filename, url)) == 0
        return filename
    
    
def load_w2v(tokenizer=None):
    word2vec_vectors = os.path.join('generated', 'GoogleNews-vectors-negative300.bin')
    word2vec_vectors
        
    w2v_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_vectors, binary=True)
    
    total_count = sum(tokenizer.word_counts.values())
    idf_dict = { k: np.log(total_count/v) for (k,v) in tokenizer.word_counts.items() }
    
    w2v = np.zeros((tokenizer.num_words, w2v_model.wv.syn0.shape[1]))
    idf = np.zeros((tokenizer.num_words, 1))

    for k, v in tokenizer.word_index.items():
        if v >= tokenizer.num_words:
            continue

        if k in w2v_model:
            w2v[v] = w2v_model[k]
            idf[v] = idf_dict[k]

    del w2v_model
    return w2v, idf
```


```python
# This may take a while to load

w2v, idf = nb_utils.load_w2v(tokenizer)
```


```python
tokens = tokenizer.texts_to_sequences(emotion_df['content'])
tokens = pad_sequences(tokens)

training_count = 36000

training_tokens, training_labels = tokens[:training_count], labels[:training_count]
test_tokens, test_labels = tokens[training_count:], labels[training_count:]
```


```python
from keras import layers, models
import keras.backend as K


def make_embedding(name, vocab_size, embedding_size, weights=None, mask_zero=True):
    if weights is not None:
        return layers.Embedding(mask_zero=mask_zero, input_dim=vocab_size, 
                                output_dim=weights.shape[1], 
                                weights=[weights], trainable=False, 
                                name='%s/embedding' % name)
    else:
        return layers.Embedding(mask_zero=mask_zero, input_dim=vocab_size, 
                                output_dim=embedding_size,
                                name='%s/embedding' % name)

def create_unigram_model(vocab_size, embedding_size=None, embedding_weights=None, idf_weights=None):
    assert not (embedding_size is None and embedding_weights is None)
    message = layers.Input(shape=(None,), dtype='int32', name='message')
    
    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights)
    idf = make_embedding('message_idf', vocab_size, embedding_size, idf_weights)

    mask = layers.Masking(mask_value=0)
    def _combine_and_sum(args):
        embedding, idf = args
        return K.sum(embedding * K.abs(idf), axis=1)

    sum_layer = layers.Lambda(_combine_and_sum, name='combine_and_sum')
    sum_msg = sum_layer([mask(embedding(message)), idf(message)])
    fc1 = layers.Dense(units=128, activation='relu')(sum_msg)
    categories = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(fc1)
    
    model = models.Model(
        inputs=[message],
        outputs=categories,
    )
    
    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
    model.summary()
    return model

unigram_model = create_unigram_model(vocab_size=VOCAB_SIZE,
                                     embedding_weights=w2v,
                                     idf_weights=idf)
```

    Model: "model_2"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    message (InputLayer)            (None, None)         0                                            
    __________________________________________________________________________________________________
    message_vec/embedding (Embeddin (None, None, 300)    15000000    message[0][0]                    
    __________________________________________________________________________________________________
    masking_1 (Masking)             (None, None, 300)    0           message_vec/embedding[0][0]      
    __________________________________________________________________________________________________
    message_idf/embedding (Embeddin (None, None, 1)      50000       message[0][0]                    
    __________________________________________________________________________________________________
    combine_and_sum (Lambda)        (None, 300)          0           masking_1[0][0]                  
                                                                     message_idf/embedding[0][0]      
    __________________________________________________________________________________________________
    dense_3 (Dense)                 (None, 128)          38528       combine_and_sum[0][0]            
    __________________________________________________________________________________________________
    dense_4 (Dense)                 (None, 13)           1677        dense_3[0][0]                    
    ==================================================================================================
    Total params: 15,090,205
    Trainable params: 40,205
    Non-trainable params: 15,050,000
    __________________________________________________________________________________________________



```python
unigram_model.fit(training_tokens, training_labels, epochs=10)
```

    Epoch 1/10
    36000/36000 [==============================] - 3s 95us/step - loss: 2.2920 - accuracy: 0.2833
    Epoch 2/10
    36000/36000 [==============================] - 3s 94us/step - loss: 1.9916 - accuracy: 0.3271
    Epoch 3/10
    36000/36000 [==============================] - 3s 95us/step - loss: 1.9480 - accuracy: 0.3424
    Epoch 4/10
    36000/36000 [==============================] - 3s 92us/step - loss: 1.9155 - accuracy: 0.3492
    Epoch 5/10
    36000/36000 [==============================] - 3s 91us/step - loss: 1.8904 - accuracy: 0.3573
    Epoch 6/10
    36000/36000 [==============================] - 3s 83us/step - loss: 1.8709 - accuracy: 0.3660
    Epoch 7/10
    36000/36000 [==============================] - 3s 93us/step - loss: 1.8534 - accuracy: 0.3708
    Epoch 8/10
    36000/36000 [==============================] - 3s 91us/step - loss: 1.8303 - accuracy: 0.3775
    Epoch 9/10
    36000/36000 [==============================] - 3s 94us/step - loss: 1.8145 - accuracy: 0.3811
    Epoch 10/10
    36000/36000 [==============================] - 3s 94us/step - loss: 1.8000 - accuracy: 0.3865
    
    <keras.callbacks.callbacks.History at 0x7f57fc47be10>


```python
unigram_model.evaluate(test_tokens, test_labels, verbose=2)
```


    [2.5194117879867552, 0.30524998903274536]

## Learning Embeddings

It looks like our model with pre-trained embeddings isn't doing much better than the linear models.

We can also try training a model "from scratch", and learn the word embeddings from our training data.  Note that we use a small embedding size here to speed up training and to try to avoid overfitting.

Only training for 10 epochs stops the model while it is still improving on the training set, but prevents it
from overfitting.  We can formalize this by using a validation set and early stopping.


```python
learned_embeddings_model = create_unigram_model(vocab_size=VOCAB_SIZE, embedding_size=25)
```

    Model: "model_3"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    message (InputLayer)            (None, None)         0                                            
    __________________________________________________________________________________________________
    message_vec/embedding (Embeddin (None, None, 25)     1250000     message[0][0]                    
    __________________________________________________________________________________________________
    masking_2 (Masking)             (None, None, 25)     0           message_vec/embedding[0][0]      
    __________________________________________________________________________________________________
    message_idf/embedding (Embeddin (None, None, 25)     1250000     message[0][0]                    
    __________________________________________________________________________________________________
    combine_and_sum (Lambda)        (None, 25)           0           masking_2[0][0]                  
                                                                     message_idf/embedding[0][0]      
    __________________________________________________________________________________________________
    dense_5 (Dense)                 (None, 128)          3328        combine_and_sum[0][0]            
    __________________________________________________________________________________________________
    dense_6 (Dense)                 (None, 13)           1677        dense_5[0][0]                    
    ==================================================================================================
    Total params: 2,505,005
    Trainable params: 2,505,005
    Non-trainable params: 0
    __________________________________________________________________________________________________

```python
learned_embeddings_model.fit(training_tokens, training_labels, epochs=10, batch_size=128)
```

    Epoch 1/10
    36000/36000 [==============================] - 1s 38us/step - loss: 2.1414 - accuracy: 0.2535
    Epoch 2/10
    36000/36000 [==============================] - 1s 35us/step - loss: 1.9853 - accuracy: 0.3136
    Epoch 3/10
    36000/36000 [==============================] - 1s 33us/step - loss: 1.8861 - accuracy: 0.3542
    Epoch 4/10
    36000/36000 [==============================] - 1s 33us/step - loss: 1.8210 - accuracy: 0.3841
    Epoch 5/10
    36000/36000 [==============================] - 1s 33us/step - loss: 1.7681 - accuracy: 0.4025
    Epoch 6/10
    36000/36000 [==============================] - 1s 33us/step - loss: 1.7175 - accuracy: 0.4224
    Epoch 7/10
    36000/36000 [==============================] - 1s 33us/step - loss: 1.6656 - accuracy: 0.4396
    Epoch 8/10
    36000/36000 [==============================] - 1s 34us/step - loss: 1.6119 - accuracy: 0.4588
    Epoch 9/10
    36000/36000 [==============================] - 1s 34us/step - loss: 1.5565 - accuracy: 0.4776
    Epoch 10/10
    36000/36000 [==============================] - 1s 34us/step - loss: 1.5002 - accuracy: 0.4999
    
    <keras.callbacks.callbacks.History at 0x7f51b92bedd0>


```python
# Note the test set accuracy is lower than that on the training set.

learned_embeddings_model.evaluate(test_tokens, test_labels, verbose=2)
```


    [2.0114533109664916, 0.3504999876022339]

# 更多复杂的模型

As with our previous task, we can try using more powerful models to classify our text.  In this case, the limited training data and text size limit their effectiveness.


```python
def create_cnn_model(vocab_size, embedding_size=None, embedding_weights=None):
    message = layers.Input(shape=(None,), dtype='int32', name='title')
    
    # The convolution layer in keras does not support masking, so we just allow
    # the embedding layer to learn an explicit value.
    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights,
                              mask_zero=False)

    def _combine_sum(v):
        return K.sum(v, axis=1)

    cnn_1 = layers.Convolution1D(128, 3)
    cnn_2 = layers.Convolution1D(128, 3)
    cnn_3 = layers.Convolution1D(128, 3)
    
    global_pool = layers.GlobalMaxPooling1D()
    local_pool = layers.MaxPooling1D(strides=1, pool_size=3)

    cnn_encoding = global_pool(cnn_3(local_pool(cnn_2(local_pool(cnn_1(embedding(message)))))))
    fc1 = layers.Dense(units=128, activation='elu')(cnn_encoding)
    categories = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(fc1)
    model = models.Model(
        inputs=[message],
        outputs=[categories],
    )
    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
    return model
```


```python
cnn_model = create_cnn_model(VOCAB_SIZE, embedding_weights=w2v)
cnn_model.summary()
```

    Model: "model_4"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    title (InputLayer)              (None, None)         0                                            
    __________________________________________________________________________________________________
    message_vec/embedding (Embeddin (None, None, 300)    15000000    title[0][0]                      
    __________________________________________________________________________________________________
    conv1d_7 (Conv1D)               (None, None, 128)    115328      message_vec/embedding[0][0]      
    __________________________________________________________________________________________________
    max_pooling1d_7 (MaxPooling1D)  (None, None, 128)    0           conv1d_7[0][0]                   
                                                                     conv1d_8[0][0]                   
    __________________________________________________________________________________________________
    conv1d_8 (Conv1D)               (None, None, 128)    49280       max_pooling1d_7[0][0]            
    __________________________________________________________________________________________________
    conv1d_9 (Conv1D)               (None, None, 128)    49280       max_pooling1d_7[1][0]            
    __________________________________________________________________________________________________
    global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_9[0][0]                   
    __________________________________________________________________________________________________
    dense_7 (Dense)                 (None, 128)          16512       global_max_pooling1d_1[0][0]     
    __________________________________________________________________________________________________
    dense_8 (Dense)                 (None, 13)           1677        dense_7[0][0]                    
    ==================================================================================================
    Total params: 15,232,077
    Trainable params: 232,077
    Non-trainable params: 15,000,000
    __________________________________________________________________________________________________

```python
cnn_model.fit(training_tokens, training_labels, epochs=10)
```

    Epoch 1/10
    36000/36000 [==============================] - 7s 184us/step - loss: 1.9732 - accuracy: 0.3205
    Epoch 2/10
    36000/36000 [==============================] - 7s 182us/step - loss: 1.8570 - accuracy: 0.3603
    Epoch 3/10
    36000/36000 [==============================] - 6s 178us/step - loss: 1.7910 - accuracy: 0.3849
    Epoch 4/10
    36000/36000 [==============================] - 6s 176us/step - loss: 1.7103 - accuracy: 0.4116
    Epoch 5/10
    36000/36000 [==============================] - 7s 181us/step - loss: 1.6169 - accuracy: 0.4436
    Epoch 6/10
    36000/36000 [==============================] - 6s 180us/step - loss: 1.4999 - accuracy: 0.4841
    Epoch 7/10
    36000/36000 [==============================] - 6s 176us/step - loss: 1.3797 - accuracy: 0.5255
    Epoch 8/10
    36000/36000 [==============================] - 6s 171us/step - loss: 1.2600 - accuracy: 0.5654
    Epoch 9/10
    36000/36000 [==============================] - 7s 181us/step - loss: 1.1517 - accuracy: 0.6024
    Epoch 10/10
    36000/36000 [==============================] - 7s 183us/step - loss: 1.0570 - accuracy: 0.6345
    
    <keras.callbacks.callbacks.History at 0x7f57a46a4f90>


```python
cnn_model.evaluate(test_tokens, test_labels)
```

    4000/4000 [==============================] - 0s 94us/step
    
    [2.7884333724975585, 0.3240000009536743]

## 使用卷积神经网络进行情感分类


```python
def create_lstm_model(vocab_size, embedding_size=None, embedding_weights=None):
    message = layers.Input(shape=(None,), dtype='int32', name='title')
    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights)(message)

    lstm_1 = layers.LSTM(units=128, return_sequences=False)(embedding)
#     lstm_2 = layers.LSTM(units=128, return_sequences=False)(lstm_1)
    category = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(lstm_1)
    
    model = models.Model(
        inputs=[message],
        outputs=[category],
    )
    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
    return model
```


```python
lstm_model = create_lstm_model(VOCAB_SIZE, embedding_weights=w2v)
lstm_model.summary()
```

    WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.where in 2.0, which has the same broadcast rule as np.where
    Model: "model_5"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    title (InputLayer)           (None, None)              0         
    _________________________________________________________________
    message_vec/embedding (Embed (None, None, 300)         15000000  
    _________________________________________________________________
    lstm_1 (LSTM)                (None, 128)               219648    
    _________________________________________________________________
    dense_9 (Dense)              (None, 13)                1677      
    =================================================================
    Total params: 15,221,325
    Trainable params: 221,325
    Non-trainable params: 15,000,000
    _________________________________________________________________

```python
lstm_model.fit(training_tokens, training_labels, epochs=10, batch_size=128)
```

    Epoch 1/10
    36000/36000 [==============================] - 12s 335us/step - loss: 2.0157 - accuracy: 0.3120
    Epoch 2/10
    36000/36000 [==============================] - 12s 322us/step - loss: 1.8867 - accuracy: 0.3525
    Epoch 3/10
    36000/36000 [==============================] - 12s 326us/step - loss: 1.8432 - accuracy: 0.3683
    Epoch 4/10
    36000/36000 [==============================] - 12s 324us/step - loss: 1.8134 - accuracy: 0.3780
    Epoch 5/10
    36000/36000 [==============================] - 12s 323us/step - loss: 1.7899 - accuracy: 0.3844
    Epoch 6/10
    36000/36000 [==============================] - 12s 323us/step - loss: 1.7667 - accuracy: 0.3905
    Epoch 7/10
    36000/36000 [==============================] - 12s 324us/step - loss: 1.7423 - accuracy: 0.4003
    Epoch 8/10
    36000/36000 [==============================] - 12s 325us/step - loss: 1.7183 - accuracy: 0.4063
    Epoch 9/10
    36000/36000 [==============================] - 12s 323us/step - loss: 1.6943 - accuracy: 0.4154
    Epoch 10/10
    36000/36000 [==============================] - 12s 323us/step - loss: 1.6656 - accuracy: 0.4230
    
    <keras.callbacks.callbacks.History at 0x7f579440bc50>


```python
lstm_model.evaluate(test_tokens, test_labels)
```

    4000/4000 [==============================] - 2s 504us/step
    
    [1.8894655408859253, 0.3709999918937683]



## Comparing our models

Let's compare the predictions from our models on a sample of our data.


```python
predictions = {
    'lstm': lstm_model.predict(test_tokens[:100]),
    'char_cnn': char_cnn_model.predict(test_char_vectors[:100]),
    'cnn': cnn_model.predict(test_tokens[:100]),
    'unigram': unigram_model.predict(test_tokens[:100]),
}
```


```python
# Make a dataframe just for test data

pd.options.display.max_colwidth = 128
test_df = emotion_df[training_count:training_count+100].reset_index()
eval_df = pd.DataFrame({
    'content': test_df['content'],
    'true': test_df['sentiment'],
    'lstm': [label_encoder.classes_[np.argmax(x)] for x in predictions['lstm']],
    'cnn': [label_encoder.classes_[np.argmax(x)] for x in predictions['cnn']],
    'char_cnn': [label_encoder.classes_[np.argmax(x)] for x in predictions['char_cnn']],    
    'unigram': [label_encoder.classes_[np.argmax(x)] for x in predictions['unigram']],
})
eval_df = eval_df[['content', 'true', 'lstm', 'cnn', 'char_cnn', 'unigram']]
eval_df.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
    .dataframe tbody tr th {
        vertical-align: top;
    }
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>content</th>
      <th>true</th>
      <th>lstm</th>
      <th>cnn</th>
      <th>char_cnn</th>
      <th>unigram</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>HAPPY MOTHER'S DAY to all of the wonderful women out there.  Have a great and relaxful day.</td>
      <td>happiness</td>
      <td>happiness</td>
      <td>love</td>
      <td>happiness</td>
      <td>love</td>
    </tr>
    <tr>
      <th>1</th>
      <td>browsing thru adopting agencies, i'm gonna get some exotic kids</td>
      <td>enthusiasm</td>
      <td>neutral</td>
      <td>worry</td>
      <td>worry</td>
      <td>happiness</td>
    </tr>
    <tr>
      <th>2</th>
      <td>I am tired of my phone. Walkman works like a charm, but l need better video and wap really. Thanks for yesterday and for buy...</td>
      <td>love</td>
      <td>relief</td>
      <td>neutral</td>
      <td>sadness</td>
      <td>love</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Happy Mother's Day to all the Mommiessss</td>
      <td>love</td>
      <td>happiness</td>
      <td>love</td>
      <td>love</td>
      <td>love</td>
    </tr>
    <tr>
      <th>4</th>
      <td>@mattgarner haha what's up Matt ?</td>
      <td>happiness</td>
      <td>neutral</td>
      <td>happiness</td>
      <td>neutral</td>
      <td>worry</td>
    </tr>
    <tr>
      <th>5</th>
      <td>What's up!!? @guillermop</td>
      <td>neutral</td>
      <td>neutral</td>
      <td>neutral</td>
      <td>neutral</td>
      <td>neutral</td>
    </tr>
    <tr>
      <th>6</th>
      <td>@KandyBee we shuld do  a dance like that its seriously the best thing haha. see yu tomoro.</td>
      <td>fun</td>
      <td>happiness</td>
      <td>happiness</td>
      <td>worry</td>
      <td>worry</td>
    </tr>
    <tr>
      <th>7</th>
      <td>@TravelTweetie I will go to sleep now. Might be awakened early w/breakfast tray from my 'spark' &amp;amp; my 'joper' w/their Dad...</td>
      <td>happiness</td>
      <td>worry</td>
      <td>neutral</td>
      <td>worry</td>
      <td>neutral</td>
    </tr>
    <tr>
      <th>8</th>
      <td>@nak1a &amp;quot;If there's a camel up a hill&amp;quot; and &amp;quot;I'll give you plankton&amp;quot; ....HILARIOUS!!</td>
      <td>happiness</td>
      <td>happiness</td>
      <td>neutral</td>
      <td>worry</td>
      <td>worry</td>
    </tr>
    <tr>
      <th>9</th>
      <td>@Bern_morley LOL I love your kids</td>
      <td>love</td>
      <td>love</td>
      <td>love</td>
      <td>love</td>
      <td>love</td>
    </tr>
  </tbody>
</table>
</div>



## Qualitative Evaluation

We can examine some of our error cases by hand.  Often, the models tend to agree when they make mistakes, and that the mistakes aren't unreasonable: this task would be challenging even for a human.


```python
eval_df[eval_df['lstm'] != eval_df['true']].head(10)
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
    .dataframe tbody tr th {
        vertical-align: top;
    }
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>content</th>
      <th>true</th>
      <th>lstm</th>
      <th>cnn</th>
      <th>char_cnn</th>
      <th>unigram</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>browsing thru adopting agencies, i'm gonna get some exotic kids</td>
      <td>enthusiasm</td>
      <td>neutral</td>
      <td>worry</td>
      <td>worry</td>
      <td>happiness</td>
    </tr>
    <tr>
      <th>2</th>
      <td>I am tired of my phone. Walkman works like a charm, but l need better video and wap really. Thanks for yesterday and for buy...</td>
      <td>love</td>
      <td>relief</td>
      <td>neutral</td>
      <td>sadness</td>
      <td>love</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Happy Mother's Day to all the Mommiessss</td>
      <td>love</td>
      <td>happiness</td>
      <td>love</td>
      <td>love</td>
      <td>love</td>
    </tr>
    <tr>
      <th>4</th>
      <td>@mattgarner haha what's up Matt ?</td>
      <td>happiness</td>
      <td>neutral</td>
      <td>happiness</td>
      <td>neutral</td>
      <td>worry</td>
    </tr>
    <tr>
      <th>6</th>
      <td>@KandyBee we shuld do  a dance like that its seriously the best thing haha. see yu tomoro.</td>
      <td>fun</td>
      <td>happiness</td>
      <td>happiness</td>
      <td>worry</td>
      <td>worry</td>
    </tr>
    <tr>
      <th>7</th>
      <td>@TravelTweetie I will go to sleep now. Might be awakened early w/breakfast tray from my 'spark' &amp;amp; my 'joper' w/their Dad...</td>
      <td>happiness</td>
      <td>worry</td>
      <td>neutral</td>
      <td>worry</td>
      <td>neutral</td>
    </tr>
    <tr>
      <th>10</th>
      <td>@davecandoit dude that honest to god happens to me all the time.. minus the trail mix.</td>
      <td>sadness</td>
      <td>neutral</td>
      <td>worry</td>
      <td>worry</td>
      <td>happiness</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Happy Mother's Day to the tweetin' mamas  Nite tweeple!</td>
      <td>worry</td>
      <td>love</td>
      <td>love</td>
      <td>love</td>
      <td>love</td>
    </tr>
    <tr>
      <th>13</th>
      <td>On my way home...then SLEEP! Seeing Amber Pacific tomorow with the besties</td>
      <td>happiness</td>
      <td>neutral</td>
      <td>happiness</td>
      <td>happiness</td>
      <td>happiness</td>
    </tr>
    <tr>
      <th>14</th>
      <td>@xoMusicLoverxo I'm using it in a story. I actually already wrote it but have to write the chapters before it.</td>
      <td>relief</td>
      <td>worry</td>
      <td>neutral</td>
      <td>sadness</td>
      <td>neutral</td>
    </tr>
  </tbody>
</table>
</div>
